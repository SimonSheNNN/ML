{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMMASUUdm4E7",
        "colab_type": "text"
      },
      "source": [
        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqy1LtZCIQuG",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import libraries\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfePG6PyjNp",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2lGPqwTymkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c90c0be-4f19-4007-bc9d-13ad32780fae"
      },
      "source": [
        "url=\"https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/diabetes.csv\"\n",
        "response = requests.get(url).content\n",
        "diabetes = pd.read_csv(io.StringIO(response.decode('utf-8')))\n",
        "diabetes.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGU3nr9PzMq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training, validation and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = diabetes.to_numpy()\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, stratify=y, test_size = 0.25, random_state=66)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.4, random_state=66)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HMQDN_YzIQuK"
      },
      "source": [
        "# KNN Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WgH67-bV4AJ9"
      },
      "source": [
        "We will implement the KNN algorithm for the diabetes dataset. Refer to the pdf and the following functions for the instructions. Complete all the functions as indicated below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qmuIim2IQuK",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Task 1: Classification\n",
        "\n",
        "Please implement KNN for K: 3, 5, and 7 \n",
        "with the following norms:\n",
        "L1\n",
        "L2\n",
        "L-inf\n",
        "\n",
        "\"\"\"\n",
        "def distanceFunc(metric_type, vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the distance between two d-dim vectors\n",
        "    Args:\n",
        "        metric_type: String\n",
        "        vec1 (numpy vector): Vector\n",
        "        vec2 (numpy vector): Vector\n",
        "    Returns:\n",
        "        distance (float): distance between the two vectors\n",
        "    \"\"\"\n",
        "\n",
        "    diff = vec1 - vec2\n",
        "    if metric_type == \"L1\":\n",
        "      distance=sum(abs(diff))\n",
        "        #complete\n",
        "\n",
        "    if metric_type == \"L2\":\n",
        "      distance=math.sqrt(sum(abs(np.power(diff,2)))) \n",
        "        #complete\n",
        "        \n",
        "    if metric_type == \"L-inf\":\n",
        "      distance=max(abs(diff))\n",
        "        #complete\n",
        "        \n",
        "    return distance\n",
        "\n",
        "\n",
        "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
        "    \"\"\"\n",
        "    Compute the distances between every datapoint in the train_data and the \n",
        "    given sample. Then, find the k-nearest neighbors.\n",
        "    Return a numpy array of the label of the k-nearest neighbors.\n",
        "    \n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (String): metric type\n",
        "        X_train (numpy array): Training data\n",
        "        y_train : Training labels\n",
        "        sample (numpy vector): Sample whose distance is to computed with every entry in the dataset\n",
        "        \n",
        "    Returns:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "    \"\"\"\n",
        "    all_label=[]\n",
        "\n",
        "    #get the distance from the sample to each point in training and append to list\n",
        "    for i in range(len(X_train)):\n",
        "      all_label.append(tuple([distanceFunc(metric_type, X_train[i], sample),y_train[i]]))     \n",
        "    #sort by distance, select k of them, and get their labels \n",
        "    all_label=sorted(all_label, key = lambda x: x[0])\n",
        "    k_label=all_label[:K]\n",
        "    neighbors=[x[1] for x in k_label]\n",
        "\n,
        "\n",
        "    return neighbors\n",
        "\n",
        "def Majority(neighbors):\n",
        "    \"\"\"\n",
        "    Performs majority voting and returns the predicted value for the test sample\n",
        "    Args:\n",
        "        neighbors (list): K-nearest neighbors' labels\n",
        "    Returns:\n",
        "        predicted_value (int or float): predicted label for the given sample\n",
        "    \"\"\"\n",
        "    #get the majority using max of count\n",
        "    predicted_value=max(set(neighbors), key = neighbors.count)\n",
        "    # Performs majority voting\n",
        "    # Complete this function\n",
        "    \n",
        "    \n",
        "    \n",
        "    return predicted_value\n",
        "\n",
        "\n",
        "def KNN(K, metric_type, X_train, y_train, X_val):\n",
        "    \"\"\"\n",
        "    Returns the predicted values for the entire validation or test set\n",
        "    Args:\n",
        "        K (int): K-value\n",
        "        metric_type (String): metric type\n",
        "        X_train (numpy array): Training data\n",
        "        y_train : Training labels\n",
        "        X_val or X_test (numpy array): Validation or test data\n",
        "    Returns:\n",
        "        predicted_values (list): output for every entry in validation/test dataset \n",
        "    \"\"\"\n",
        "    predictions=list()\n",
        "    for i in range(len(X_val)):\n",
        "      predictions.append(Majority(computeDistancesNeighbors(K, metric_type, X_train, y_train, X_val[i])))\n",
        "    # Complete this function\n",
        "    # Loop through the val_data or the test_data (as required)\n",
        "    # and compute the output for every entry in that dataset  \n",
        "    \n",
        "    \n",
        "        \n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluation(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: vector\n",
        "        actual_values: numpy vector\n",
        "    \n",
        "    Returns:\n",
        "        accuracy (float): accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Calls the above functions in order to implement the KNN algorithm.\n",
        "    \n",
        "    Test over the following range K = 3,5,7 and all three metrics. \n",
        "    \n",
        "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
        "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Complete this function\n",
        "    \n",
        "    K = [3,5,7]\n",
        "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
        "    \n",
        "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
        "    colK=list()\n",
        "    colnorm=list()\n",
        "    colaccuracy=list()\n",
        "    # for every k, every norm, use KNN function to generate labels for the validation set and calculate accuracy\n",
        "    for i in K:\n",
        "      for j in norm:\n",
        "        colK.append(i)\n",
        "        colnorm.append(j)\n",
        "        colaccuracy.append(accuracy_score(KNN(i, j, X_train, y_train, X_val),y_val))\n",
        "        \n",
        "    df = pd.DataFrame(list(zip(colK, colnorm,colaccuracy)),columns=['K', 'Norm','Accuracy'])\n",
        "    print (df)\n",
        "    ## Complete\n",
        "\n",
        "    print(\"<<<<TEST DATA PREDICTIONS>>>>\")\n",
        "    \n",
        "    #best parameters are: K=7, norm=L2\n",
        "    accuracy_test=(accuracy_score(KNN(7, 'L2', X_train, y_train, X_test),y_test))\n",
        "        \n",
        "   \n",
        "    print (accuracy_test)\n",
        "    ## Complete\n",
        "    "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4D4AbBjhK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "311d4a40-c76e-492b-9048-195da11fce81"
      },
      "source": [
        "# Finally, call the main function\n",
        "main()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<<<<VALIDATION DATA PREDICTIONS>>>>\n",
            "   K   Norm  Accuracy\n",
            "0  3     L1  0.721739\n",
            "1  3     L2  0.695652\n",
            "2  3  L-inf  0.739130\n",
            "3  5     L1  0.756522\n",
            "4  5     L2  0.765217\n",
            "5  5  L-inf  0.704348\n",
            "6  7     L1  0.730435\n",
            "7  7     L2  0.773913\n",
            "8  7  L-inf  0.695652\n",
            "<<<<TEST DATA PREDICTIONS>>>>\n",
            "0.7142857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XmURULzoXCvq"
      },
      "source": [
        "# Decision Tree Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S-_QAM3jXCvq"
      },
      "source": [
        "### Helper functions\n",
        "The block below contains helper functions for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9KC2aNmoXCvr",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "  \"\"\"\n",
        "  Trains a decision tree classifier on the given X, y data with the specified \n",
        "  tree depth d and max leaf node count max_leaf_num.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "             on the depth of the tree.\n",
        "    leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "    no restrictions on the leaf count of the tree.\n",
        "  \n",
        "  Returns:\n",
        "    clf(DecisionTreeClassifier): the trained decision tree classifier\n",
        "  \"\"\"\n",
        "  clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def predict(clf, X_test):\n",
        "  \"\"\"\n",
        "  Uses a trained decision tree classifier to predict on a given test set.\n",
        "  \n",
        "  Args:\n",
        "    clf (DecisionTreeClassifier): Trained Decision Tree Classifer\n",
        "    X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                               p (number of features) matrix\n",
        "  \n",
        "  Returns:\n",
        "    y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "  \"\"\"\n",
        "  y_pred = clf.predict(X_test)\n",
        "  return y_pred\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    return accuracy_score(predicted_values, actual_values)\n",
        "  \n",
        "def plot_line_graph(x_vals, y_vals_1, y_vals_2, y_vals_1_label, y_vals_2_label, x_axis_label, y_axis_label, title):\n",
        "  \"\"\"\n",
        "  Plots a line graph of two lines of different values with common x-values\n",
        "  \n",
        "  Args:\n",
        "    x_vals ((j,) list): Values to be displayed on horizontal axis, where j is number of values\n",
        "    y_vals_1 ((j,) list): First set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_2 ((j,) list): Second set of values to be graphed on a line in respect to x_vals, where j is number of values\n",
        "    y_vals_1_label (string): Label for first set of y values\n",
        "    y_vals_2_label (string): Label for second set of y values\n",
        "    x_axis_label (string): Label for x axis\n",
        "    y_axis_label (string): Label for y axis\n",
        "    title (string): Plot title\n",
        "  \"\"\"\n",
        "  \n",
        "  plt.plot(x_vals, y_vals_1, color='g', label=y_vals_1_label)\n",
        "  plt.plot(x_vals, y_vals_2, color='orange', label=y_vals_2_label)\n",
        "  plt.xlabel(x_axis_label)\n",
        "  plt.ylabel(y_axis_label)\n",
        "  plt.title(title)\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eP9j8K21XCvs"
      },
      "source": [
        "### Compare Accuracy for full classification dataset as well as smaller classification dataset\n",
        "\n",
        "To start, uncomment the code below and run to create small dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LRothkZeXCvt",
        "colab": {}
      },
      "source": [
        "# We will also use the same diabetes classification dataset in Task 1.\n",
        "# Let's create a smaller version of the training dataset using only half of the data available\n",
        "\n",
        "train_sample_num_small = int(X_train.shape[0] / 2)\n",
        "X_train_small, y_train_small = X_train[:train_sample_num_small], y_train[:train_sample_num_small]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEsSQwRMXCvv"
      },
      "source": [
        "### Base Metrics on Full and Partial Data\n",
        "\n",
        "Note: Make sure to create two separate classifiers for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xfRXN92DXCvv",
        "colab": {}
      },
      "source": [
        "def base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Create a decision tree classifer on the full dataset and the partial dataset (only half of n).\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray): Input feature matrix of full dataset for training/fitting\n",
        "    y_train ((n,) np.ndarray): Input label array of full dataset for training/fitting\n",
        "    X_train_small ((n,p) np.ndarray): Input feature matrix of partial/small dataset for training/fitting\n",
        "    y_train_small ((n,) np.ndarray): Input label array of partial/small dataset for training/fitting\n",
        "    X_val ((n,p) np.ndarray): Input feature matrix of full dataset for validation\n",
        "    y_val ((n,) np.ndarray): Input label array of full dataset for validation\n",
        "    X_test ((n,p) np.ndarray): Input feature matrix of full dataset for testing\n",
        "    y_test ((n,) np.ndarray): Input label array of full dataset for testing    \n",
        "\n",
        "  To observe:\n",
        "    train_acc_full_set (float): Training accuracy using a model trained on the full dataset\n",
        "    val_acc_full_set (float): Validation accuracy using a model trained on the full dataset\n",
        "    test_acc_full_set (float): Test accuracy using a model trained on the full dataset\n",
        "    train_acc_small_set (float): Training accuracy using a model trained on the small dataset\n",
        "    val_acc_small_set (float): Validation accuracy using a model trained on the full dataset\n",
        "    test_acc_small_set (float): Test accuracy using a model trained on the full dataset\n",
        "  \"\"\"\n",
        "  \n",
        "  train_acc_full_set=evaluate(predict(train_decision_tree(X_train,y_train),X_train),y_train)\n",
        "  val_acc_full_set=evaluate(predict(train_decision_tree(X_train,y_train),X_val),y_val)\n",
        "  test_acc_full_set=evaluate(predict(train_decision_tree(X_train,y_train),X_test),y_test)\n",
        "  \n",
        "  train_acc_small_set=evaluate(predict(train_decision_tree(X_train_small,y_train_small),X_train_small),y_train_small)\n",
        "  val_acc_small_set=evaluate(predict(train_decision_tree(X_train_small,y_train_small),X_val),y_val)\n",
        "  test_acc_small_set=evaluate(predict(train_decision_tree(X_train_small,y_train_small),X_test),y_test)\n",
        "  \n",
        "\n",
        "  print(\"Train Accuracy on Full Dataset: \", train_acc_full_set)\n",
        "  print(\"Validation Accuracy on Full Dataset: \", val_acc_full_set)\n",
        "  print(\"Test Accuracy on Full Dataset: \", test_acc_full_set)\n",
        "  print(\"Train Accuracy on Small (Half) Dataset: \", train_acc_small_set)\n",
        "  print(\"Validation Accuracy on Small (Half) Dataset: \", val_acc_small_set)\n",
        "  print(\"Test Accuracy on Small (Half) Dataset: \", test_acc_small_set)\n",
        "  \n",
        "  return (train_acc_full_set, \n",
        "          val_acc_full_set, \n",
        "          test_acc_full_set,\n",
        "          train_acc_small_set, \n",
        "          val_acc_small_set, \n",
        "          test_acc_small_set)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QTH2bUBY3SCP"
      },
      "source": [
        "Uncomment the code below and run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VM9BV2i3XCvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8f7937b7-0d50-4495-cdab-472febd29c5a"
      },
      "source": [
        "base_metrics(X_train, y_train, X_train_small, y_train_small, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy on Full Dataset:  1.0\n",
            "Validation Accuracy on Full Dataset:  0.7043478260869566\n",
            "Test Accuracy on Full Dataset:  0.7532467532467533\n",
            "Train Accuracy on Small (Half) Dataset:  1.0\n",
            "Validation Accuracy on Small (Half) Dataset:  0.7130434782608696\n",
            "Test Accuracy on Small (Half) Dataset:  0.6753246753246753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0,\n",
              " 0.7043478260869566,\n",
              " 0.7532467532467533,\n",
              " 1.0,\n",
              " 0.7130434782608696,\n",
              " 0.6753246753246753)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7gUUoJSO3SCR"
      },
      "source": [
        
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6xiF5-LvXCvz"
      },
      "source": [
        "### Improving Decision Tree for Smaller Dataset by Tuning Hyperparameters\n",
        "Classifiers often overfit on smaller datasets, so now, we will optimize hyperparameters on tree depth and max leaf count to improve the performance of our model. \n",
        "\n",
        "Fill out the helper functions below which will take an array of hyperparameter values for tree depth and an array of hyperparameter values for max leaf count. The helper function will return a training and validation accuracy scores for each pair of hyperparameter values. This is referred to as grid search for hyperparameter tuning.\n",
        "\n",
        "At the end, the function identifies the best value of the tree depth and tree node count hyperparameters for a dataset, as well as the final training and testing scores.\n",
        "\n",
        "Note: Use the highest validation score to choose the optimal hyperparameter combination. If there is a tie, use the lower hyperparameter value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1nPIPFmXCvz",
        "colab": {}
      },
      "source": [
        "def grid_search_depth_and_leaf_count(depth_search_space, leaf_count_search_space, X_train, y_train, X_val, y_val):\n",
        "  \"\"\"\n",
        "  Perform a decision tree hyperparameter grid search on tree depth and leaf count given training and validation data.\n",
        "    \n",
        "  Args:\n",
        "    depth_search_space ((d,) list): Tree depth values to search over, i.e. [1, 3, 6, 10, 30]\n",
        "    leaf_count_search_space ((l,) list): Max leaf count values to search over, i.e. [2, 3, 4, 5, 6]\n",
        "    X_train ((n, p) np.ndarray): The input feature matrix for training\n",
        "    y_train ((n, p) np.ndarray): The input ys for training\n",
        "    X_val ((n, p) np.ndarray): The input feature matrix that will be used to validate accuracy scores\n",
        "    y_val ((n, p) np.ndarray): The input ys that will be used to validate accuracy scores\n",
        "    \n",
        "  To return:\n",
        "    best_depth (int): The depth count in the hyperparameter combination with the largest validation score\n",
        "    best_leaf_count (int): The leaf count in the hyperparameter combination with the largest validation score\n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  \n",
        "  train_accuracy=list()\n",
        "  val_accuracy=list()\n",
        "  depth=list()\n",
        "  leaf=list()\n",
        "  for i in depth_search_space:\n",
        "    for j in leaf_count_search_space:\n",
        "        depth.append(i)\n",
        "        leaf.append(j)\n",
        "        train_accuracy.append(evaluate(predict(train_decision_tree(X_train,y_train,depth=i, leaf_count=j),X_train),y_train))\n",
        "        val_accuracy.append(evaluate(predict(train_decision_tree(X_train,y_train,depth=i, leaf_count=j),X_val),y_val))\n",
        "  df = pd.DataFrame(list(zip(depth, leaf,train_accuracy,val_accuracy)))\n",
        "  \n",
        "  best_depth=df.loc[df[3]==max(df[3]), 0].iloc[0]\n",
        "  best_leaf_count=df.loc[df[3]==max(df[3]), 1].iloc[0]\n",
        "\n",
        "  \n",
        "  print(\"Chosen Depth: \", best_depth)\n",
        "  print(\"Chosen Leaf: \", best_leaf_count)\n",
        "  \n",
        "  return best_depth, best_leaf_count"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RvCBkKLJ3SCV"
      },
      "source": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RyZZ2CHb3SCW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "059de72f-5add-4a4d-861d-0ca612cd1abf"
      },
      "source": [
        "# Search spaces for grid search to tune tree depth and leaf count hyperparameters\n",
        "depth_search_space = [1, 3, 6, 10, 30]\n",
        "leaf_count_search_space = [2, 3, 4, 5, 6]\n",
        "\n",
        "print(\"FULL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train, \n",
        "                                 y_train, \n",
        "                                 X_val, \n",
        "                                 y_val)\n",
        "\n",
        "print(\"\\nSMALL DATASET\")\n",
        "grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                 leaf_count_search_space, \n",
        "                                 X_train_small, \n",
        "                                 y_train_small, \n",
        "                                 X_val,\n",
        "                                 y_val)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL DATASET\n",
            "Chosen Depth:  3\n",
            "Chosen Leaf:  4\n",
            "\n",
            "SMALL DATASET\n",
            "Chosen Depth:  1\n",
            "Chosen Leaf:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T_oY8swF3SCX"
      },
      "source": [
        
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e85OLhHfXCv_"
      },
      "source": [
        "### Retrain Decision Tree and Plot Hyperparameter Search\n",
        "\n",
        "Also for the small dataset, create a graph plotting the training and validation scores for each leaf node hyperparameter value, holding the tree depth hyperparameter consistent at the chosen value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JrWrji_iXCwA",
        "colab": {}
      },
      "source": [
        "def retrain_decision_tree(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "  \n",
        "  \"\"\"\n",
        "  Perform a decision tree hyperparameter grid search given training and validation data and search values for\n",
        "  tree depth and leaf node count.\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray)\n",
        "    y_train ((n,) np.ndarray)\n",
        "    X_val ((n,p) np.ndarray)\n",
        "    y_val ((n,) np.ndarray)\n",
        "    X_test ((n,p) np.ndarray)\n",
        "    y_test ((n,) np.ndarray)\n",
        "\n",
        "  To return:\n",
        "    train_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    val_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    test_acc (float): Optimal Hyperparameters Train Accuracy\n",
        "    \n",
        "    leaf_count_train_scores (list): Report training scores for max leaf count search space\n",
        "    leaf_count_val_scores (list): Report validation scores for max leaf count search space\n",
        "  \"\"\"\n",
        "  # Select best hyperparameters\n",
        "  depth_search_space = [2, 4, 6, 8, 10, 16, 20]\n",
        "  leaf_count_search_space = [2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
        "\n",
        "  chosen_depth, chosen_leaf_count = grid_search_depth_and_leaf_count(depth_search_space, \n",
        "                                                                     leaf_count_search_space, \n",
        "                                                                     X_train, \n",
        "                                                                     y_train, \n",
        "                                                                     X_val, \n",
        "                                                                     y_val)\n",
        "  \n",
        "  train_acc=evaluate(predict(train_decision_tree(X_train,y_train,depth=chosen_depth, leaf_count=chosen_leaf_count),X_train),y_train)\n",
        "  val_acc=evaluate(predict(train_decision_tree(X_train,y_train,depth=chosen_depth, leaf_count=chosen_leaf_count),X_val),y_val)\n",
        "  test_acc=evaluate(predict(train_decision_tree(X_train,y_train,depth=chosen_depth, leaf_count=chosen_leaf_count),X_test),y_test)\n",
        "\n",
        "  leaf_count_train_scores=list()\n",
        "  leaf_count_val_scores=list()\n",
        "\n",
        "  for i in leaf_count_search_space:\n",
        "\n",
        "        leaf_count_train_scores.append(evaluate(predict(train_decision_tree(X_train,y_train,depth=chosen_depth, leaf_count=i),X_train),y_train))\n",
        "        leaf_count_val_scores.append(evaluate(predict(train_decision_tree(X_train,y_train,depth=chosen_depth, leaf_count=i),X_val),y_val))\n",
        "  plt.plot(leaf_count_search_space,leaf_count_train_scores)\n",
        "  plt.plot(leaf_count_search_space,leaf_count_val_scores)\n",
        "  \n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Leaf node')\n",
        "  plt.title('Leaf Node vs. Accuracy')\n",
        "  plt.legend(['Train','validation'])\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Optimal Hyperparameters Train Accuracy: \", train_acc)\n",
        "  print(\"Optimal Hyperparameters Validation Accuracy: \", val_acc)\n",
        "  print(\"Optimal Hyperparameters Test Accuracy: \", test_acc)\n",
        "  \n",
        "  print(\"Training Scores per Max Leaf Node Count:\", leaf_count_train_scores)\n",
        "  print(\"Validation Scores per Max Leaf Node Count:\", leaf_count_val_scores)\n",
        "  \n",
        "  return (train_acc, val_acc, test_acc, leaf_count_train_scores, leaf_count_val_scores)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F0BXjFYN3SCa"
      },
      "source": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jLJCMcNWXCwD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "f6b2eee3-faef-43bd-82cd-4dae57a7141a"
      },
      "source": [
        "retrain_decision_tree(X_train_small, y_train_small, X_val, y_val, X_test, y_test)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chosen Depth:  6\n",
            "Chosen Leaf:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimal Hyperparameters Train Accuracy:  0.8159722222222222\n",
            "Optimal Hyperparameters Validation Accuracy:  0.7739130434782608\n",
            "Optimal Hyperparameters Test Accuracy:  0.7142857142857143\n",
            "Training Scores per Max Leaf Node Count: [0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7743055555555556, 0.7777777777777778, 0.7951388888888888, 0.8159722222222222, 0.8159722222222222]\n",
            "Validation Scores per Max Leaf Node Count: [0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7478260869565218, 0.7130434782608696, 0.7739130434782608, 0.7739130434782608]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8159722222222222,\n",
              " 0.7739130434782608,\n",
              " 0.7142857142857143,\n",
              " [0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7743055555555556,\n",
              "  0.7777777777777778,\n",
              "  0.7951388888888888,\n",
              "  0.8159722222222222,\n",
              "  0.8159722222222222],\n",
              " [0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7478260869565218,\n",
              "  0.7130434782608696,\n",
              "  0.7739130434782608,\n",
              "  0.7739130434782608])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0-UXUer3SCd"
      },
      "source": [
       
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbNi5JkX0HnG",
        "colab_type": "text"
      },
      "source": [
        "## Additional Exercise (Ungraded)\n",
        "\n",
        "This section is ungraded. You can try it out for fun. sklearn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) used in this homework has a parameter called **min_samples_split**. Try playing around with it's values to see if you can get better accuracy for the X_train_small dataset. \n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2xmO04DXCwF"
      },
      "source": [
        "#Feature Scaling Effects on KNNs and DTs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NG7pZLBoXCwG"
      },
      "source": [
        "### Observing effects of standardizing features\n",
        "\n",
        "Up until now, we have not been using standardized features. Let's observe the effects of standardized features with decision trees and KNNs.\n",
        "\n",
        "Standardization, or feature scaling / data normalization, is a common preprocessing step for data within machine learning. We will see why it's important.\n",
        "\n",
        "Here is a definition taken from SK-Learn's website on Standardization:\n",
        "\n",
        "*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.*\n",
        "\n",
        "*In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.*\n",
        "\n",
        "Learn More: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "To start, uncomment the code below and run to retrieve the data. (Recomment before submission.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T5RokryGXCwH",
        "colab": {}
      },
      "source": [
        "# We will use the same data as the previous tasks\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9COC_Pa3SCg"
      },
      "source": [
        "### Helper Functions\n",
        "  \n",
        "We implemented above the KNN algorithm. Sci-kit learn also has their own version of the KNN algorithm which we will use in this following task. Use the two helper functions below in this next task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LJvoFpqh3SCg",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def train_KNN(X, y, norm=2, K=5):\n",
        "  \"\"\"\n",
        "  Trains a KNN classifier on the given X, y data with the specified \n",
        "  norm and K.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    norm (int): The number form of the norm. Note that sklearn only allows L1 and L2 norms,\n",
        "                (norm would be 1 and 2 respectively). Default is 2.\n",
        "    K (int): The value of K for the KNN algorithm. Default is 5.\n",
        "  \n",
        "  Returns:\n",
        "    clf(KNeighborsClassifier): the trained KNN model\n",
        "  \"\"\"\n",
        "  \n",
        "  clf = KNeighborsClassifier(n_neighbors=K, p=norm)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def train_decision_tree(X, y, depth=None, leaf_count=None):\n",
        "  \"\"\"\n",
        "  This helper function is defined again from a previous section. \n",
        "  \n",
        "  Trains a decision tree classifier on the given X, y data with the specified \n",
        "  tree depth d and max leaf node count max_leaf_num.\n",
        "  \n",
        "  Args:\n",
        "    X ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                          p (number of features) matrix\n",
        "    y ((n,) np.ndarray): The input ys, which are in an n length array\n",
        "    depth (int): The maximum depth of the tree. A value of None means no restrictions\n",
        "             on the depth of the tree.\n",
        "    leaf_count (int): The maximum leaf count of the tree's leaf nodes. A value of None means \n",
        "    no restrictions on the leaf count of the tree.\n",
        "  \n",
        "  Returns:\n",
        "    clf(DecisionTreeClassifier): the trained decision tree\n",
        "  \"\"\"\n",
        "  clf = DecisionTreeClassifier(max_depth=depth, max_leaf_nodes=leaf_count, criterion=\"entropy\", random_state=1)\n",
        "  clf.fit(X,y)\n",
        "  return clf\n",
        "\n",
        "def predict(clf, X_test):\n",
        "  \"\"\"\n",
        "  This helper function is defined again from a previous section. \n",
        "  \n",
        "  Uses a trained model to predict on a given test set.\n",
        "  \n",
        "  Args:\n",
        "    clf (Classifier): Trained classifier such as KNN or Decision Tree\n",
        "    X_test ((n,p) np.ndarray): The input Xs, which are in an n (number of samples) by\n",
        "                               p (number of features) matrix\n",
        "  \n",
        "  Returns:\n",
        "    y_pred ((n,) np.ndarray): The output predictions, which are in an n length array\n",
        "  \"\"\"\n",
        "  y_pred = clf.predict(X_test)\n",
        "  return y_pred\n",
        "\n",
        "def evaluate(predicted_values, actual_values):\n",
        "    \"\"\"\n",
        "    This helper function is defined again from a previous section. \n",
        "    \n",
        "    Computes the accuracy of the given datapoints.\n",
        "    \n",
        "    Args:\n",
        "        predicted_values: numpy array\n",
        "        actual_values: numpy array\n",
        "    \n",
        "    Returns:\n",
        "        a floating point number representing the accuracy\n",
        "    \"\"\"\n",
        "    return accuracy_score(predicted_values, actual_values)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iVfyIhUiXCwK"
      },
      "source": [
        "### Retrieving Metrics for Unstandardized Data\n",
        "Fill out this function to retrieve training and test accuracies for both KNN and decision tree models. Use default hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0XxTvHMXCwM",
        "colab": {}
      },
      "source": [
        "def get_classifier_metrics(X_train, y_train, X_test, y_test):\n",
        "  \"\"\"\n",
        "  Create a decision tree and KNN classifer on the normal dataset.\n",
        "    \n",
        "  Args: (Note that n is not the same among train and test sets, \n",
        "         but merely refers to sample size)\n",
        "    X_train ((n,p) np.ndarray)\n",
        "    y_train ((n,) np.ndarray)\n",
        "    X_test ((n,p) np.ndarray)\n",
        "    y_test ((n,) np.ndarray)\n",
        "\n",
        "  To return:\n",
        "    knn_train_accuracy (float): Accuracy of KNN for train set\n",
        "    knn_test_accuracy (float): Accuracy of KNN for test set\n",
        "    dt_train_accuracy (float): Accuracy of DT for train set\n",
        "    dt_test_accuracy (float): Accuracy of DT for test set    \n",
        "  \"\"\"\n",
        "  \n",
        "  dt_train_accuracy=evaluate(predict(train_decision_tree(X_train,y_train),X_train),y_train)\n",
        "  dt_test_accuracy=evaluate(predict(train_decision_tree(X_train,y_train),X_test),y_test)\n",
        "  knn_train_accuracy=evaluate(predict(train_KNN(X_train,y_train),X_train),y_train)\n",
        "  knn_test_accuracy=evaluate(predict(train_KNN(X_train,y_train),X_test),y_test)\n",
        "  \n",
        "  print(\"knn_train_accuracy: \", knn_train_accuracy)\n",
        "  print(\"knn_test_accuracy: \", knn_test_accuracy)\n",
        "  print(\"dt_train_accuracy: \", dt_train_accuracy)\n",
        "  print(\"dt_test_accuracy: \", dt_test_accuracy)\n",
        "  \n",
        "  return knn_train_accuracy, knn_test_accuracy, dt_train_accuracy, dt_test_accuracy"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF3Jmuy53SCk"
      },
      "source": [
        "Uncomment the code below and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kWS33o6OXCwO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5fc8d135-dc2d-4e60-f930-3cbb6315a9ce"
      },
      "source": [
        "print(\"FOR UNSTANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nFOR STANDARDIZED DATA\")\n",
        "get_classifier_metrics(X_train_scaled, y_train, X_test_scaled, y_test)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOR UNSTANDARDIZED DATA\n",
            "knn_train_accuracy:  0.7899305555555556\n",
            "knn_test_accuracy:  0.7012987012987013\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.7532467532467533\n",
            "\n",
            "FOR STANDARDIZED DATA\n",
            "knn_train_accuracy:  0.8177083333333334\n",
            "knn_test_accuracy:  0.8181818181818182\n",
            "dt_train_accuracy:  1.0\n",
            "dt_test_accuracy:  0.7532467532467533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8177083333333334, 0.8181818181818182, 1.0, 0.7532467532467533)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZCmPf0G3SCn"
      },
      "source": [
       
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-7QVbownVl4",
        "colab_type": "text"
      },
      "source": [

        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
      ]
    }
  ]
}
